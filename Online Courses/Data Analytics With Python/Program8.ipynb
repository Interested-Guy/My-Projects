{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Program8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVazpOFSyfyN"
      },
      "source": [
        "##Simple Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IIyaO_YxVDD"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('toyota.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPqW_3I-ymQI"
      },
      "source": [
        "#Let us look at the correlation between various variables in the toyota dataframe\n",
        "df.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtkLwLPP0_eE"
      },
      "source": [
        "###The correlation matrix displays correlation coefficients for all pairs of variables.\n",
        "###The correlation coefficient shows how strongly two variables. It can be +ve(increasing effect) or -ve(decreasing effect).\n",
        "###We saw that there is a negative correlation between price of a car and its age i.e. as age increases the price decreases. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofjkbeHq2H_v"
      },
      "source": [
        "#To visualize the correlation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "sns.regplot(x=df['Age'],y=df['Price'],fit_reg=True,color='green')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsyya7tgBx-g"
      },
      "source": [
        "###A regression model describes how y(Price) is related to x(Age). A simple linear regression model is of the form\n",
        "###y=b0+b1*x\n",
        "###The above equation represents a straight line whose slope is b1 and y intercept is b0. Thus the equation helps us to predict values of y corresponding to some value of x. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rpi2rGBEFPE"
      },
      "source": [
        "#Let us look how we can get a linear regression model for x=Age and y=Price\n",
        "import statsmodels.api as s\n",
        "x=list(df['Age'])\n",
        "y=list(df['Price'])\n",
        "x=s.add_constant(x)\n",
        "model=s.OLS(y,x)\n",
        "result=model.fit()\n",
        "result.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBijuXcLGQ1b"
      },
      "source": [
        "###The coef column in the above result contains y intercept(const) and the slope(x1). Besides we can see various other terms like R^2, F-statistic, t- statistic etc.\n",
        "###These terms are useful when we do not consider the entire population but only the samples. In this case, the linear regression model obtained is called estimated lrm. \n",
        "###To check if the estimated slope holds good for the entire population we perform t test(for two variables) or f test(anova)(generally used when there are more than two variables).\n",
        "###The null hypothesis is:H0:B0=0 where B0 is the slope corresponding to the population.\n",
        "###If the null hypothesis becomes true, then it means that there is no relation between y and x at the population level.\n",
        "###The term R^2 is called coefficient of determination. It describes how well the model performs. It basically checks if the model's error is explained by variation of x alone or not. If R^2 is 1 it means the model is perfect i.e. it gives exact values of y corresponding to x. If R^2 is zero, it means that there is no relation between x and y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S62bQjYYJlcL"
      },
      "source": [
        "#Let us perform the same operation using sklearn module\n",
        "import sklearn.model_selection as sk\n",
        "x=df['Age'].values.reshape(-1,1)     #Converting data into 2D arrays\n",
        "y=df['Price'].values.reshape(-1,1)\n",
        "x_train,x_test,y_train,y_test=sk.train_test_split(x,y,test_size=.2,random_state=88) \n",
        "#The data is split into two parts:train and test\n",
        "#train is used to train the model while test is used to check the accuracy of the model. \n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg=LinearRegression()\n",
        "reg.fit(x_train,y_train) #Prepare the model\n",
        "#Display information\n",
        "print(\"Y Intercept=\",reg.intercept_)\n",
        "print(\"Slope=\",reg.coef_)\n",
        "print(\"Score(R^2)=\",reg.score(x_test,y_test))\n",
        "y_predict=reg.predict(x_test)\n",
        "print(\"x value\\tPredicted y value\\tActual y value\")\n",
        "for i in range(10):\n",
        "  print(x_test[i],\"\\t\",y_predict[i],\"\\t\",y_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0fVb77byZZU"
      },
      "source": [
        "#Now suppose we consider only samples instead of the populations.\n",
        "#Let the sample size be 200\n",
        "sns.regplot(x=df['Age'][:200],y=df['Price'][:200],fit_reg=True,color='green')\n",
        "plt.scatter(np.mean(df['Age'][:200]),np.mean(df['Price'][:200]),color='red')  #This line shows the mean of the data as red dots\n",
        "plt.show()\n",
        "#Notice the light colored boundary around the regression line. \n",
        "#This is a Confidence interval for the mean value of y for a given value of x\n",
        "#This applies when we look at samples instead of entire populations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehbJHg2MznDO"
      },
      "source": [
        "#Prepare model for sample\n",
        "import random as rd\n",
        "x=rd.sample(list(df['Age']),200)\n",
        "y=rd.sample(list(df['Price']),200)\n",
        "x1=x.copy()\n",
        "y1=y.copy()\n",
        "x=s.add_constant(x)\n",
        "model=s.OLS(y,x)\n",
        "result=model.fit()\n",
        "\n",
        "#Preparing prediction intervals, confidence intervals\n",
        "from statsmodels.stats.outliers_influence import summary_table\n",
        "st,data1,ss2=summary_table(result,alpha=0.05)\n",
        "fittedvalues=data1[:,2]\n",
        "ci_low,ci_up=data1[:,4:6].T\n",
        "pred_low,pred_up=data1[:,6:8].T\n",
        "\n",
        "#Displaying the graph with prediction and confidence intervals.\n",
        "X=s.add_constant(x)\n",
        "fig,ax=plt.subplots(figsize=(8,6))\n",
        "ax.plot(x,y,'o',label='df')\n",
        "ax.plot(X,fittedvalues,'r-',label='OLS')\n",
        "ax.plot(X,ci_low,'b--',label=\"Conf. Int\")\n",
        "ax.plot(X,ci_up,'b--',label='Conf. Int')\n",
        "ax.plot(X,pred_low,'g--',label=\"Pred. Int\")\n",
        "ax.plot(X,pred_up,'g--',label=\"Pred. Int\")\n",
        "ax.legend(loc='best');\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Residual Analysis:To determine if the assumptions that we make about the error term in the linear regression model are true. The assumptions are:\n",
        "###1.Normal distribution\n",
        "###2.Mean is Zero\n",
        "###3.Independent and\n",
        "###4.Variance is same for all x terms\n",
        "###We must check the validity of these assumptions because they are the ground for the t test and f test which were discussed above. If the assumptions are questionable, then the test itself is compromised.\n",
        "###We make use of residual(error) plots to determine the validity of the assumptions."
      ],
      "metadata": {
        "id": "lnhHMp8Ar0pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Residual Plot of Residual against x\n",
        "sns.residplot(x=x1,y=y1)\n",
        "plt.show()\n",
        "#If the pattern is more or less rectangular,it indicates the variance of error is nearly same for all x values\n",
        "#Else variance is not same.\n",
        "\n",
        "#The residual plot of residual against y^ is also similar but more widely used because, \n",
        "#in the case of multiple regression, we have to draw seperate plots for each variable. This can be avoided by\n",
        "#using a single plot of residual against y^"
      ],
      "metadata": {
        "id": "X7zWPn4Zofdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Next we need to check if the residuals have a normal distribution or not.\n",
        "#For that we use standardized residuals.\n",
        "#If most of these(95%) residuals fall within -2 and 2, then we can say that residuals have normal dis.\n",
        "#Otherwise they do not.\n",
        "influence=result.get_influence()\n",
        "resid_student=influence.resid_studentized_external\n",
        "c=0\n",
        "#finding what percentage of values lie between -2 and 2\n",
        "for i in resid_student:\n",
        "  if(i>-2 and i<2):\n",
        "    c=c+1\n",
        "plt.title(\"Percentage of Values between -2 and +2=\"+str((c/len(resid_student))*100))\n",
        "plt.scatter(x1,resid_student)\n",
        "plt.show()\n",
        "#It can be observed that the two plots(residual and standard-residual) are exactly same but only the y axis has been standardized."
      ],
      "metadata": {
        "id": "TJpxVMoarEeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Another way to assess normailty is to use the mormal probability plot.\n",
        "#In this plot the standardized residuals are compared with similar normal scores from a normal distribution.\n",
        "#(where mean=0 and standard deviation is 1)\n",
        "#If this plot is inclined at approximately 45 degrees to the x axis, then the residuals are said to be normally distributed.\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "res=result.resid\n",
        "prbplot=sm.ProbPlot(res,stats.norm,fit=True)\n",
        "fig=prbplot.qqplot(line='45')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R5ByPeduPZW-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}